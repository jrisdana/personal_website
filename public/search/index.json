[{"content":"Correlation is everywhere in scientific analysis, but what does it mean when variables are “correlated”? What does correlation mean anyway?\rThis article addresses common misconceptions about correlation and their consequences in academia and beyond. I’ve struggled with some of these misconceptions, so I thought it would be best to define correlation and explain how to use it.\nDefining Correlation\rWhen doing a quick Google search on Correlation, the first definition is:\n“a mutual relationship or connection between two or more things”\rIn fact, most websites define correlation as a “technique that shows how strongly pairs of variables are related”. I think it’s safe to define Correlation as the association between two variables. However, the most popular correlation measure, the Pearson Correlation, has a very different definition.\rPearson correlation is often confused with the general definition of correlation.\n\rPearson Correlation\rPearson Correlation measures the strength of a linear relationship between variables/features. The measurement is expressed as the Pearson Correlation Coefficient, which ranges in value from -1 to +1. The closer the magnitude is to 1, the stronger the linear relationship.\nHow does it work?\rLet’s look at the formula for the population correation coefficient to understand how it’s measured:\r\\[ corr_{xy} = \\frac{Cov(X, Y)}{\\sigma_x \\sigma_y}\\]\rI used the population correlation coefficient instead of the sample correlation coefficient because I wanted to better highlight its relationship with other statistical concepts. Let’s define some of the variables and measures in this formula:\n\r\\(X, Y\\) are random variables - or a set of numbers that are outcomes of a random phenomenon. These variables come from some sort of probability distribution. The Pearson Correlation assumes these variables come from a Normal distribution. That is, \\(X\\) and \\(Y\\) are assumed to have values symmetric about the mean, and more values close to the mean than far from the mean. This creates a Bell curve, with the height of the curve being at the mean.\n\r\\(\\sigma_x, \\sigma_y\\) are the standard deviations of \\(X\\) and \\(Y\\). That is, the measure of how far apart values are relative to its mean. This is called spread.\n\r\\(Cov(X, Y)\\) is the covariance of the two random variables. Covariance is the measure of how much one variable changes relative to another.\rLet’s look at the Covariance formula for more information:\r\\[ Cov(X, Y) = E(X - E[X])(Y-E[Y])\\]\rCovariance measures the average change between two variables. A positive covariance means that the positive values of \\(X\\) correspond with the positive values of \\(Y\\) on average. Likewise, the negative values of \\(X\\) correspond to the negative values of \\(Y\\) on average. As one variable increases, the other also increases on average.\rFor intuition, let’s look at a graph. The Ecdat package provides data sets for econometrics.\r\r\rThe relationship between Bid Price and Ask Price has a positive covariance. \\(E[X]\\) is the blue vertical line and \\(E[Y]\\) is the red horizontal line. When \\(X\\) (Ask price) is less than \\(E[X]\\) (mean of Ask price), or left of the blue line, \\(Y\\) (Bid price) is also less than \\(E[Y]\\) (Mean of bid price). Thus, \\(X - E[X]\\) and \\(Y-E[Y]\\) are both negative when \\(X\\) and \\(Y\\) are less than their means. The product of these differences is positive. \\(X - E[X]\\) and \\(Y - E[Y]\\) are both positive when \\(X\\) and \\(Y\\) are greater than their means. The product of these differences are also positive. Thus, the average of the variable’s differences, or the Covariance, is always positive.\nA negative covariance means that as one variable increases, the other decreases on average. The product of the average \\(X\\) and \\(Y\\) differences from their means will be negative. A near zero covariance means that the variables do not fluctuate together proportionally or inversely on average. Covariance is only effective when the relationships are proportional or inversely proportional - meaning we can only detect linear relationships.\rWhile covariance can tell us whether variables have a positive or negative linear relationship, it doesn’t give us reliable information on the relationship’s strength.\n\rBy Strength, we mean how close the two variables follow a straight line.\n\rCovariance is measured in the units of the selected variables. If we’re measuring something in feet, find the covariance, and convert feet to meters, the magnitude (size) of the covariance changes. Since the size of the covariance is affected by the size of the units, the strength of a linear relationship remains unclaer.\nThe correlation formula “normalizes” the covariance formula, meaning that it gets rid of the units and scales the measure from [0,1]. This is done by dividing the Covariance by the product of standard deviations, \\(\\sigma_x \\sigma_y\\), of each variable. To better understand this, let’s look at the correlation formula again in terms of expectations:\r\\[ Cor(X,Y) = \\frac{E(X-E[X])(Y-E[Y])}{\\sqrt{(E(X-E[X])^2)\\cdot(E(Y-E[Y])^2)}}\\]\rThis is messy, but it illustrates this: What if \\(X\\) and \\(Y\\) changed at a constant proportion at all data points? That is,\r\\[X - E[X] = Y - E[Y] \\]\rIf we were to substitute this in and simplify, we would get\r\\[ Cor(X, Y) = \\frac{E(X-E[X])^2}{E(X-E[X])^2} = 1 \\]\rIf \\(X\\) and \\(Y\\) fluctuated perfectly inverse of each other, the correlation coefficient would be \\(-1\\). These are the maximum and minimum of the possible range for a correlation coefficient. If the correlation coefficient nears 0, the linear relationship is weak. When \\(X\\) and \\(Y\\) do not consistently change together, the correlation coefficient will be somewhere between 0.00 and 0.99. How do we know this?\n\rCauchy-Schwarz Inequality\rThe Cauchy-Schwarz Inequality is a very useful inequality that says that, if \\(X\\) and \\(Y\\) are random variables and \\(E(XY)\\) exists,\r\\[E(XY)^2 \\leq E(X^2)E(Y^2)\\]\rSince \\(X\\) can be any random variable, we can set \\(X\\) to be \\(X - E[X]\\) and \\(Y\\) to be \\(Y - E[Y]\\). According to Cauchy-Schwarz inequality,\r\\[E[(X-E[X]))(Y-E[Y])]^2 \\leq E((X-E[X])^2)E((Y-E[Y])^2)\\]\rThis is equivalent to\r\\[ Cov(X,Y)^2 \\leq Var(X)Var(Y)\\]\rTaking the square root of both sides, we find that\r\\[ |Cov(X,Y)| \\leq \\sigma(X)\\sigma(Y)\\]\r\\(\\sigma(\\cdot)\\) being the Standard deviation, which is the square root of the Variance.\rThis verifies the claim that\r\\[\\Big|\\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}\\Big| = |Corr(X,Y)| \\leq 1\\]\rWe know that covariance can either be positive or negative, so we know that\r\\[ \\rho(X,Y) \\in [-1,1] \\]\r\\(\\rho\\) being shorthand for Pearson Correlation.\n\r\rWhy did I talk about this?\rThe article only talks about misuses of Pearson correlation.\rFor example, one misconception from Dr. Velickovic’s article is that a correlation coefficient of zero implies independence. While independent random variables are uncorrelated, the converse is not true.\rAdditionally, Pearson correlation is only valid for detecting linear relationships between variables. Nonlinear relationships, like parabolas, could yield a zero correlation coefficient. While the correlation may be zero, there does exist a relationship between random variables.\rPearson correlation is rooted in linearity, and it cannot detect nonlinear relationships.\rSo what else can we use? If we want to evaluate the relationship between two variables, but can’t assume that our data is linear, what can we use?\nSpearman’s Correlation\rSpearman’s correlation measure cannot detect nonmonotonic relationships, so something like a parabola, where variables move positively and negatively together, would still yield a zero correlation coefficient.\n\rNOTE: There is a correlation measure that can detect nonlinear relationships and independence - Distance Correlation. A Distance correlation of zero implies independence. However, I want to write about Similarity and Distance first before writing about Distance Correlation. I’ll link that article here once it’s published.\n\rHowever, Spearman’s Correlation can detect nonlinear monotonic relationships - think exponential, logistic curves, etc. It is a nonparametric measure of correlation, meaning that it doesn’t assume the data comes from a known distribution. To understand the significance of this, let’s look at the assumptions made using Pearson’s correlation:\n\rBoth variables are normally distributed (bivariate normally distributed)\n\rBoth variables follow a linear relationship (linearity)\n\rThe spread of data points is the same in both random variables (homoscedasticity)\n\r\rThese are some restrictive assumptions - we can only use continuous data with constant variance. Parametric data is great for powerful results, but real-world data usually violates at least one of these assumptions.\nLet’s look at an exponential scatterplot and see how both Pearson and Spearman correlation performs.\nx \u0026lt;- c(1,2,3,4,5,6,7,8,9, 10,\r11,12,13,14,15,16,17,18)\ry \u0026lt;- exp(x)\rdf \u0026lt;- cbind(x,y)\rdf \u0026lt;- as.data.frame(df)\rg \u0026lt;- ggplot(df, aes(x,y)) + geom_point() + geom_smooth(method=\u0026quot;lm\u0026quot;, se=FALSE)\rg\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rLet’s use the Pearson correlation coefficient formula to find the strength of linear relationship.\ncov(x,y)/(sd(x)*sd(y))\r## [1] 0.56\rPearson correlation indicates a positive linear relationship between our sample data. We know that this isn’t true, since we plotted an exponential curve. However, we do know that the variables move in the same direction.\rSpearman’s Rank correlation takes the Pearson correlation of the rank variables.\n\rA Rank variable is the ordered values of a numeric variable. We order values from largest to smallest. We assign our largest value of Rank 1, second-largest value of Rank 2, and so on.\n\rIn our case, our highest rank will be Rank 1 and our lowest rank will be rank 18. So our formula should look like:\r\\[r_s = \\frac{Cov(rank_x,rank_y)}{\\sigma({rank_x})\\sigma({rank_y})}\\]\rLet’s create a table that ranks our columns and appends them to the dataframe.\norder.x \u0026lt;- order(x)\rorder.y \u0026lt;- order(y)\rdf$rank_x \u0026lt;- order.x\rdf$rank_y \u0026lt;- order.y\rdf.1 \u0026lt;- head(df)\rcolnames(df.1) \u0026lt;- c(\u0026quot;X\u0026quot;, \u0026quot;Y\u0026quot;, \u0026quot;Ranking of Variable X\u0026quot;, \u0026quot;Ranking of Variable Y\u0026quot;)\rdf.1 %\u0026gt;% kable() %\u0026gt;% kable_styling()\r\r\rX\r\rY\r\rRanking of Variable X\r\rRanking of Variable Y\r\r\r\r\r\r1\r\r2.7\r\r1\r\r1\r\r\r\r2\r\r7.4\r\r2\r\r2\r\r\r\r3\r\r20.1\r\r3\r\r3\r\r\r\r4\r\r54.6\r\r4\r\r4\r\r\r\r5\r\r148.4\r\r5\r\r5\r\r\r\r6\r\r403.4\r\r6\r\r6\r\r\r\r\rNow let’s compute Spearman’s correlation, or the correlation of the ranked variables.\nwith(df, cov(rank_x, rank_y)/\r(sd(rank_x)*sd(rank_y)))\r## [1] 1\rSpearman correlation indicates a perfect monotonically increasing relationship. That is, one variable either increases or stays the same as another variable increases. Spearman correlation also returns values ranging from -1 to 1. A perfect monotonically decreasing relationship is expressed as a Spearman correlation of -1.\nSpearman measures the correlations of ranked-ordered variables. The formula is exactly that of Pearson correlation, except with ranked variables. Since we’re only looking at the ranked variables, we can now find monotonic correlation of ordinal data (categorical data with a set order). Spearman’s Correlation allows us to find association between ordinal and continuous variables, rather than only continuous variables.\n\rKendall’s Tau\rLet’s recap what we know so far:\n\rPearson Correlation is the covariance of two random variables, scaled by its respective standard deviations. It measures the average linear strength between two variables with respect to their means.\n\rSpearman Correlation is a special case of the Pearson correlation, where data is ranked from largest to smallest. We apply Pearson correlation to the rank values of the variables rather than the variables itself. This allows us to measure for nonlinear monotonic relationships.\n\r\rThe Kendall Rank Correlation, or Kendall’s tau, is another rank correlation measure that can be used with continuous and ordinal variables. However, unlike Spearman’s correlation, Kendall’s Tau is not an extension of Pearson correlation.\rInstead of measuring correlation by a scaled covariance of the variables, Kendall’s tau finds association based on the concordance of pairs.\nWhat does concordance even mean?\rA Concordant pair is the number of observations that are below a rank. A Discordant pair is the number of observations that are above a rank.\nThe closer the coefficient is to 1, the closer the relationship is to perfect concordance. That is, if the ordered pairs are similar in ranking, the coefficient will be close to 1. If the pairs are inversely related, then the coefficient will be close to -1.\nLet’s look at the formula for tau:\n\\[\\hat \\tau = \\frac{C - D}{\\binom{n}{2}}\\]\rWhere\n\r\\(C\\) is the number of concordant pairs\n\r\\(D\\) is the number of discordant pairs\n\r\rand \\(\\binom{n}{2}\\) is the number of combinations that can be selected from two observations. This scales the coefficient between -1 and 1.\n\rASIDE: This requires that we know the order of variables. If we can’t decide if one rank is larger of smaller, we can’t measure concordance and discordance. We can’t measure the number of ranks below or above a data point if we don’t know how many ranks there are or what a rank means.\n\rThis might be unclear. Let’s use a dataset with an ordered column and an unordered column to measure concordance.\nOrder \u0026lt;- c(1,2,3,4,5)\rCompared \u0026lt;- c(2,3,5,1,4)\rconcordant \u0026lt;- function(x){\rlength(Order) - x\r}\rdiscordant \u0026lt;- function(x,y) {\rabs(x - y)\r}\rConcordance \u0026lt;- concordant(Compared)\rDiscordance \u0026lt;- discordant(Order, Compared)\rdf.tau \u0026lt;- as.data.frame(\rcbind(Order, Compared, Concordance, Discordance)\r)\rdf.tau %\u0026gt;% kable() %\u0026gt;%\rkable_styling(position=\u0026quot;center\u0026quot;)\r\r\rOrder\r\rCompared\r\rConcordance\r\rDiscordance\r\r\r\r\r\r1\r\r2\r\r3\r\r1\r\r\r\r2\r\r3\r\r2\r\r1\r\r\r\r3\r\r5\r\r0\r\r2\r\r\r\r4\r\r1\r\r4\r\r3\r\r\r\r5\r\r4\r\r1\r\r1\r\r\r\r\rKendall’s tau compares the number of pairs that agree (sets with more low rankings) and disagree (sets with more high rankings) and finds its ratio to the maximum number of pairs that can differ between two sets.\nInstead of using a variable’s mean, Kendall’s tau uses the difference in ranking positions in each observation. Let’s look at our tau coefficient.\r\\[ \\tau = \\frac{2}{10} = .2\\]\rThis is quite a departure from Pearson and Spearman correlation.\nAdditionally, Pearson and Spearman correlations measure the strength of a relationship by how much of the variance is explained by both variables.\nSince \\(\\tau\\) is based on the number of different pairs between two ordered sets, we can interpret Kendall’s tau as differences between the probability of concordance and the probability of discordance. If this doesn’t make sense, think of tau as the ratio of frequency of similar rankings to all possible pairs of rankings.\nThis is interesting! Different types of correlation can detect different relationships, use different statistical tools, and are still considered a measure of correlation.\n\r\rMultiple interpretations of Correlation\r\rNOTE: A faster way to find Spearman and Kendall Tau correlation is to use the cor function in R and set “method=spearman” or “method=kendall”. I didn’t go into this, but the Kendall tau formula is different when there are tied ranks. I’ll talk about the differences between Kendall tau correlations soon.\n\rThe difference between Spearman’s Correlation and Kendall Tau is an important reminder: correlation is simply defined as an associated between variables. Correlation can use different concepts of center and still find “an association between two variables and its direction”. Another important point is that correlation is broad and many correlation techniques exist. While Pearson and Spearman correlation assumes a specific underlying relationship between variables (linear and monotonic), Kendall’s tau assumes nothing of the shape of the relationship. Even though these correlations differ in approach, they still quantify the relationship and its direction. I’ll revisit Kendall’s tau in a separate article regarding Similarity and Distance.\nEven though each of these techniques are a measure of correlation, each correlation defines “correlated” variables differently!\n\rSignificance\rMy point is that:\n\rThe type of correlation used, and the validity of its use (whether underlying assumptions are violated), will determine the type of conclusions made.\n\rThere are many different types of correlation, and there are even more ways to measure similarity between variables.\n\rDifferent correlations can show different aspects of your data, and the types of variables in your data will determine which correlations to use.\n\r\rUnfortunately, this was another long article - I promise I will be more concise in the future.\nOne last thing! Just reiterating a point Dr. Velikovic made in their article. Correlation can only provide information on the tested variables. Variables at the individual level cannot speak for variables at the aggregate level.\nReferences\r\rHervé Abdi - The Kendall Rank Correlation Coefficient\n\rG.E.Noether - Why Kendall Tau?\n\rJerrold H. Zar - Spearman Rank Correlation\n\rJosh Starmer - Statquest\n\rStack Exchange\n\rThe BMJ. 11. Correlation and regression\n\r\r\r\r\r","date":"2020-08-05","permalink":"/post/2020-08-05-what-s-correlation-why-do-people-use-it/","tags":[],"title":"What's Correlation? Why do people use it? "},{"content":"I’m from Orange County. Orange County has been in the news a lot lately. Unfortunately, it’s because of this. Orange County has had a lot of protests to reopen the county throughout lockdown. On June 9th, Orange County’s Chief Health Officer resigned after threats and criticism by anti-Mask protesters and Orange County Supervisors. Two days later, officials ended the mask requirement mandate. However, San Diego County, which is just south of Orange County, kept mask requirements.\rSeeing that cases are now on the rise nationally and that my home has turned into the No-Mask movement’s epicenter, it seemed appropriate to look at whether there is an effect in mask requirement laws. We’ll first compare county cases over time and then outline an identification strategy that can validate a causal claim.\n\rWhy San Diego?: If we’re looking for any kind of causal effect in mask requirement laws and we’re using Orange County as our treatment, San Diego County is a good control. San Diego’s proximity to Orange County helps control for any regional/climate effects. They are similar in population size, differing only by 100,000 people. Both counties lean more conservative than other California industry centers (Los Angeles, San Francisco), and both economies rely on manufacturing and trade. Lastly, both counties experienced protests against the mask requirement order.\n\rMost importantly, both counties began to open businesses and attractions at similar times, so we can control for growth from relaxing other COVID-related restrictions.\rI’m going to use ggplot2 to graph the cases over time and cowplot to help with formatting.\nThe data comes from California Open Data Portal.\nSubsetting the Dataset\rThe dataset does not contain data before March 17th except for total COVID cases. I subsetted the data frame into two separate data frames to get county specific data. I also created a column that counts the number of days since March 18th (beginning of the dataset). Then, I used rbind to merge county-specific datasets together. I think there is an easier way to subset a dataset by using select, but I wanted to create a number-of-days column for each county. I’m sure, though, that there’s a faster way to create an index column - I’ll update this article with cleaner code at some point.\noc.covid \u0026lt;- covid[ which(covid$county ==\u0026#39;Orange\u0026#39;), ]\roc.covid$ind \u0026lt;- 1:nrow(oc.covid)\rsd.covid \u0026lt;- covid[ which(covid$county == \u0026#39;San Diego\u0026#39;),]\rsd.covid$ind \u0026lt;- 1:nrow(sd.covid)\rcomp_covid \u0026lt;- rbind(oc.covid, sd.covid)\rI also renamed the columns to follow my work.\ncolnames(comp_covid) \u0026lt;- c(\u0026quot;county\u0026quot;, \u0026quot;total_count\u0026quot;, \u0026quot;deaths\u0026quot;, \u0026quot;daily_count\u0026quot;, \u0026quot;daily_deaths\u0026quot;,\r\u0026quot;date\u0026quot;,\r\u0026quot;ind\u0026quot;)\r\rAsymptomatic Period\rI created two vertical lines on the 86th and 100th point of the x-axis in my graph.\nWhy? The x-axis denotes the number of days since March 18th. June 11th was the date when Orange County lifted the mask requirement. The two vertical lines highlight the maximum number of days for an infected person to be asymptomatic. These lines, which will look like a rectangle when graphed, start from the day officials lifted the mask mandate. In rare cases, symptoms can show up after 14 days, but the main idea behind highlighting the “Incubation Period” of the virus is that we can measure the effects from at least one day without the mask requirement.\nrect \u0026lt;- data.frame(xmin=86, xmax=100, ymin = -Inf, ymax = Inf)\r\rGraphing Total Cases\rThe black vertical dashed line is the date when Orange County’s Health Officer resigned, and the transparent rectangle is the following 14-day Coronavirus Incubation period. During the Incubation Period, we can see that the total Orange County cases curve approaches the San Diego’s total case curve at an increasing rate, indicating that more people in Orange County are contracting COVID-19 than in San Diego County. After the 100th day, total cases in Orange County surpass total cases in San Diego County. Orange County’s daily case rate continues to increase from that day.\nSome important information - Governer Newsom ordered Orange County to enforce the mask mandate on June 18th. Let’s add this to our graph.\nggplot(data=comp_covid, aes(x = ind, y = total_count)) + geom_point(aes(color = factor(county))) + geom_vline(xintercept=84, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;black\u0026quot;) + geom_vline(xintercept=92, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;green\u0026quot;) + geom_rect(data = rect, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), color=\u0026quot;cornflowerblue\u0026quot;,\ralpha = 0.1, inherit.aes = FALSE) + ggtitle(\u0026quot;Total New COVID Cases\u0026quot;) +\rtheme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;), legend.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) +\rguides(col=guide_legend(\u0026quot;County\u0026quot;)) + labs( x = \u0026quot;Number of Days since 3/18\u0026quot;,\ry = \u0026quot;Total Number of Cases\u0026quot;) \rHere’s an interesting point - COVID cases in Orange County grew at a similar rate to those of San Diego County until the end of the Incubation period. Afterwards, COVID cases in Orange County grew at a much faster rate than those of San Diego County.\rA lagging indicator is economics jargon for a variable that becomes apparent only after a large, long-term shift occured. Infections, or the case rate, is a lagging indicator - the Incubation period accounts for time when infected individuals may be asymptomatic, or unaware that they are carrying the disease. Death occurs after infection, so naturally, death is also a lagging indicator. However, we don’t know how long it takes to die from COVID - some people die within days of contracting the virus, while some others die after several months.\rWith this in mind, let’s see how the death rate compares across counties.\nggplot(data=comp_covid, aes(x = ind, y = deaths)) + geom_point(aes(color = county)) + geom_vline(xintercept=84, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;black\u0026quot;) +\rgeom_vline(xintercept=92, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;green\u0026quot;) + geom_rect(data = rect, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), color=\u0026quot;cornflowerblue\u0026quot;,\ralpha = 0.1, inherit.aes = FALSE) + ggtitle(\u0026quot;Total COVID Deaths\u0026quot;) +\rtheme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;), legend.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;)) +\rguides(col=guide_legend(\u0026quot;County\u0026quot;)) + labs(x = \u0026quot;Number of Days since 3/18\u0026quot;,\ry = \u0026quot;Total Deaths\u0026quot;)\rAgain this is quite interesting, the death rate is higher in Orange County relative to San Diego County during the Incubation Period and afterwards. By the end of the of the graph, the number of deaths in Orange County match the number of deaths in San Diego County. This is the first time that there were more total deaths in Orange County than in San Diego County.\nAnother, arguably better way to visualize COVID cases over time and account for the 14 day incubation period is to plot cases by two week periods. These periods need to be, at the very least, divided before and after the highlighted Incubation period. We can then compare the COVID case rate by Incubation periods. This helps visualize the full impact of the virus in one day, by accounting for the maximum amount of time an infected person can stay asymptomatic. This also gives us a simple and powerful visualization of COVID cases over time. Simple and legible data visualization can go a long way.\nlibrary(dplyr)\rcomp_covid_adj \u0026lt;- comp_covid %\u0026gt;% mutate(period = ceiling((ind+2)/14))\rcomp_covid_adj$period \u0026lt;- as.factor(comp_covid_adj$period)\r\rCreating a Two-Week period column\rThrough the dplyr package, I used the mutate function to divide the number of days column by 14 and round the result to the smallest integer that was greater than or equal to the result. To start an Incubation period on the date the mask requirement was lifted, I added two to the number of days.\rSince the ninth two-week period is not done yet, we will filter out data that falls into the ninth two-week period. Adding incomplete data will not allow us to analyze the relationship properly.\ncomp_covid_adj$period_int \u0026lt;- as.numeric(comp_covid_adj$period)\rcovid_rn \u0026lt;- filter(comp_covid_adj, period_int \u0026lt; 9)\rWe added a numeric period column so that we could filter the dataset using inequalities. This will allow us to use a linear graph for further visualization.\nFor now, we’ll look at period as a factor. To visualize the number of new COVID cases per period, let’s create a grouped barplot.\nggplot(data=covid_rn, aes(x = period, y = total_count, fill = county)) + geom_bar(position=\u0026quot;dodge\u0026quot;, alpha = 0.75, stat=\u0026quot;identity\u0026quot;) + geom_vline(xintercept=6, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;black\u0026quot;) + geom_vline(xintercept=6.5, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;green\u0026quot;) + geom_vline(xintercept=7, linetype=\u0026quot;dashed\u0026quot;, color = \u0026quot;black\u0026quot;) +\rggtitle(\u0026quot;Total New COVID Cases\u0026quot;) +\rtheme(plot.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;), legend.title = element_text(lineheight=.8, face=\u0026quot;bold\u0026quot;))+\rguides(col=guide_legend(\u0026quot;County\u0026quot;)) + labs(x = \u0026quot;Number of Two Week Periods since 3/18\u0026quot;,\ry = \u0026quot;Total Number of Cases\u0026quot;)\rThe 6th period begins on the date mask requirements were lifted and the 7th period begins on the last day of the Incubation period.\nThe total new cases for the 7th period only accounts for people who contracted the virus before/by the end of the incubation period starting on June 11th. People who contracted the virus after the beginning of the 6th period, but did not develop symptoms before the end the 7th period, will not be recorded as COVID cases until the 8th period.\nWhat’s great about this graph is that it gives us a better idea of the case rate in each county by smoothing out our data. Both barplots outline exponential growth, and we can clearly see that Orange County is growing at a much faster rate.\nSpecifically, this graph better highlights the dramatic increase in Orange County total cases from the 7th period to the 8th period. That is, Orange County gained more COVID cases after June 11th than did San Diego - the period after Orange County lifted its mask requirements.\n\rAside: Looking at the Daily Count\rLet’s also look at the daily count of COVID cases among both counties and figure out if there’s a better way to visualize this.\rThis graph is messy and hard to look at, but we can glean from it that daily COVID cases increased after the highlighted Incubation Period.\nLet’s look at Daily COVID cases over the beginning of Incubation periods, with the dashed lines denoting before and after the Incubation Period for June 11th. We’ll also use a grouped barplot to better organize the data points for visualization.\rThis graph shows that the daily count for COVID cases on the 6th period was similar between both counties. The Orange County daily count was higher than that of San Diego on the 7th period.\nThe first graph shows us how the amount of COVID cases changed after intervention.\nIn the second graph, we can see the difference in relative rate of cases between counties after intervention. This helps us understand the scale of the outbreak relative to San Diego.\nWe can see that, after six days without a mask mandate, Orange County had nearly three times the number of COVID cases as San Diego.\n\rRevisiting the Total Cases Graph\rLet’s see whether we can use this data to determine if there is a causal effect in mask requirement laws. To make a causal claim, we need an identification strategy. We’ll look at endogeneity concerns in this experiment and the feasibility of a Difference-in-Differences analysis.\rWhat’s Difference-in-Differences? Our COVID dataset uses longitudinal data - that is, a dataset in which we can see multiple observations of an individual at different times. Longitudinal data allows us to see outcomes and treatment for multiple periods in an experiment. Observation terms therefore account for individual effects and time effects, since observations on one person can change over time.\nTo clarify, our treatment is no mask mandate and our control is keeping the mask mandate. Our treated group is Orange County and our control group is San Diego County.\nIn order to estimate causal effect using DID, several assumptions must hold, one of them being:\n\rExchangeability - Data came from a randomized trial\r\rWe did not randomize anything in this study - COVID-19 happened and some states/counties reacted differently. Since we didn’t get this data from a randomized control trial (any study that randomly assigns individuals to treatment and control groups), we have to use the next best thing. That is, we have to find treatment and control groups that are as close as possible to each other. We also want to make sure that our treatment limits selection bias. Fortunately, our treatment doesn’t rely on whether individuals want to have no mask mandate or a mask mandate - Orange County and San Diego County officials chose to do different things during the pandemic, whether individuals want it or not.\nTreatment could be correlated with unobserved individual-level shifters. We assume these shifters are constant within-individual across time and additively shift the outcome. While San Diego and Orange County have a lot in common, we still need to account for fixed effects such as the time the data was recorded.\nIn DID, we look at parallel trends between the treated and control group before and after intervention, assuming that each trend will stay parallel. Controlling for fixed effects and other covariates, we then measure the difference between the actual treatment line and the assumed parallel line. The best way for me to understand this is through the Potential Outcomes Framework. That is,\r\\[E(Y_{i2}(1) - Y_{i1}(0)|D_i = 1) - E(Y_{i2}(0)- Y_{i1}(0)|D_i = 1)\\]\rwhere \\(i\\) is an index for individual changes and \\(1, 2\\) are time periods. When \\(Y(1)\\), the individual was treated. When \\(Y(0)|D = 1\\), we are looking at the counterfactual - that is, the outcome where a treated indivudal was not treated. Since we can’t look at at two different outcomes for an individual, we need to assume what the outcome would’ve been. This is why we need parallel trends before intervention! This might be confusing, so here’s a graph I do not own of a perfect Difference-in-Differences Estimation.\r\r\r\nThe main idea is that the difference in the control group is what the difference in the treated group would have been, had it not been for treatment.\rWhen writing out the general model, we denote\n\r\\(\\beta_0\\) as the baseline average\n\r\\(\\beta_1\\) as the time trend in the control group\n\r\\(\\beta_2\\) as the difference between groups pre-intervention\n\r\\(\\beta_3\\) as the difference in changes over time. That is, the difference in the treatment’s difference and the control’s difference over time.\n\r\rand \\[ Y_{it} = \\beta_0 + \\beta_1Time + \\beta_2Intervention + \\beta_3Time*Intervention \\]\nWe can find the Difference-in-Differences through intertemporal variation between groups. In less jargon, we can look at the dependency between Time and Intervention to identify a causal effect.\n\rContext\rIf we assume San Diego is a good reference for Orange County, and that trends the 84th day or the 86th day were nearly parallel, we could use Difference-In-Differences analysis to quantify the causal effect of lifting mask requirement after protests. We assume that, by lifting the mask requirement and undermining advice from top Health officials, the government encouraged a relaxed attitude towards combatting the virus and caused more infections.\nLet’s look at the rest of the assumptions needed to do DID analysis.\n\rPositivity - Any individual has a positive probability of receiving all values of the treatment variable.\n\rConsistency - If \\(Y_i\\) is selected for treatment, We are observing the potential outcome of \\(Y_i\\) being treated.\n\rSUTVA - The observation of one unit is unaffected by the assignment of treatments to other units.\n\rAllocation of intervention was not determined by outcome\n\r\rLet’s look at a linear graph of COVID cases over our two-week periods and evaluate whether this graph is suitable for DID.\rUnfortunately, we’ve violated some of these assumptions. In fact, we didn’t have to look at this graph to find that out.\rLet’s look at SUTVA. In context with our data, San Diego’s COVID cases should not depend on whether or not Orange County lifts the mask mandate - but it certainly could. Many people commute from Orange County to San Diego, and with businesses reopening/relaxed restrictions, Orange County folks may visit dense tourist centers, infecting many people. While San Diego may have mask mandates in place, there will still be more people with COVID in San Diego County than if San Diego County closed its border with Orange County - increasing the likelihood of San Diegans getting COVID.\rMoreover, it seems that our data is not parallel for the entire dataset. Lines seem parallel between the 2nd and 4th Incubation period, but cross between the 4th and the 6th Incubation period.\n\rMore Problems?\rThe strategy explained here is restricted to linearity. We know that the trajectory of COVID cases follow a logistic curve. This is a big problem, but this can be avoided by looking only at the regression from the 7th period to the 8th period. This will not catch the full impact of the causal effect though. However, it may be notable to at least recognize the existence of the effect.\nThere are nonlinear DID methods, but I don’t know them. I will hopefully soon, but no, I don’t know them now.\nEven though DID doesn’t seem feasible, graphs can still tell us a lot. What can we take away from these visualizations?\n\rOrange County had had less cases than San Diego for 100 days. Now, there are more 6,471 COVID cases in Orange County than in San Diego (as of July 18th). COVID cases are increasing at a faster rate than ever.\n\rAs of Early-July, Orange County has a COVID case rate nearly three times higher than that of San Diego. Before then, Orange County and San Diego had had similar rates in COVID cases. This statistic accounts for the lag in infections caused by the time it takes for a virus to become symptomatic (The frequently-mentioned-in-this-article Incubation Period).\n\r\r\r","date":"2020-07-24","permalink":"/post/2020-07-24-masks-and-covid-in-orange-county-vs-san-diego/","tags":["covid","eda","diff_in_diff","causal_inference"],"title":"Masks and COVID in Orange County vs. San Diego"},{"content":"In 2018 alone, nearly 70,000 Americans died from drug overdoses. For some perspective, the number of drug overdose deaths was four times higher in 2018 than in 1999. Nearly 70% of these deaths can be attributed to opioid abuse.\rThis isn’t America’s first opioid epidemic, nor is this the first time opioids have been widely available as medicine. Hypodermic needles provided immediate pain relief to an unprecedented number of casualties after the Civil War. By 1911, one in 400 US citizens were addicted to some form of opium. Opiate deaths reached epidemic proportions again in the 1950s.\rThe CDC considers the current opioid epidemic, or the third opioid epidemic, a combination of three epidemics distinguished by the types of opioids associated with mortality.\n\r1990s - 2010: Prescription painkillers\r\r2010 - present: Heroin\r\r2013 - present: Synthetic opioids\r\r\rMy goal in this article is to learn more about the crisis and find out how the crisis is getting worse. With the lack of real-time data available, we are forced to look at older datasets. These datasets, however, can tell us a lot about the scope and intensity of the epidemic currently. In this article, I will be using data from Accidental Drug Related Deaths from 2012-2018 in Connecticut.\n\rWhy am I looking at Connecticut? Before 2012, Opioid overdose deaths hit Connecticut similarly to the rest of the nation. From 2012 to 2016, the opioid death rate in Connecticut quadrupled from 5.7 to 24.5 per 100,000. In 2017, data placed Connecticut among the top ten states highest in opium-related death rates (30.9 per 100 000). However, a study done by Green et al, looking at state data from 1999 to 2007 showed a greater increasing mortality from opioid analgesics. Given the timing, the severity of the crisis, and the increased mortality from synthetic opioids, I figured that the data from Connecticut would provide insight on the national crisis.\n\rMy motivation: I recently took a class on nonparametric statistics and wanted to practice/retain these tools so that I could better explore data. Nonparametric methods can be used to help model-fitting and can provide valid findings without assuming parametric data, or data that comes from a known distribution.\nCleaning the Data\rThe dataset provided contains date and time in one column. Using the lubridate package, I created separate Year, Month, and Day columns. I also created a “Number of Days” column, which counted the total number of days since the data was recorded. I created this column to act as an index so that I could easily analyze the density of deaths and covariates over time.\rThe dataset also contained information for the Longitude, Latitude, and Address of Death in one column. I used the extract function from the dplyr package to create columns for Longitude and Latitude information.\r\rInvestigation\rLet’s look at the distribution of deaths over time using nonparametric density estimation.\rKernel density estimates rely heavily on the value of the smoothing parameter. Thus, a useful density estimation procedure needs a reliable method of determining the best bandwidth to use for a given set of data.\rInstead of choosing the smoothing parameter subjectively, we will look at several automated smoothing parameter selection procedures.\r\rnrd: Normal Reference - Scott(1992)\n\rnrd0: Silverman’s Rule of Thumb - Silverman(1986)\n\rSJ: Sheather-Jones - 1991\n\rucv: Unbiased Cross-validation\n\r\rThe Normal Reference procedure assumes that the general features of the density follow a Normal distribution. The accuracy of a Kernel Density estimator is measured by the Asymptotic Mean Integral of Squared Errors (AIMSE). The AIMSE of a kernel estimator, when bandwidth \\(h\\) is minimized, depends on an unknown density. Mathematically, the minimized bandwidth quantity is related to density by a real-valued quantity \\(R_2\\). In relating a “reference distribution” to the unknown density, we can replace \\(R_2\\) by its value for the reference distribution. When the density is assumed to be normal, we need to only estimate \\(\\sigma\\) using the standard deviation or IQR.\rThis approach leads to a value of the smoothing parameter that is easily calculated and works well for smooth distributions such as Normal. For reference, the minimized bandwidth is found by\r\\[h^* = \\Big(\\frac{1}{2\\sqrt{\\pi}R_2}\\Big)^\\frac{1}{5}\\frac{1}{n^\\frac{1}{5}}\\]\rwhere the Gaussian kernel is used and\r\\[R_2 = \\displaystyle\\int_{-\\infty}^{\\infty}p\u0026#39;\u0026#39;(y)^2dy \\sim 1.059\\frac{\\sigma}{n^\\frac{1}{5}}\\]\rThe optimal bandwidth is a function of the integrated squared second derivative. The constant, \\(1.059\\), simply comes from using a Gaussian Kernel and assuming the density follows a Normal distribution. In many cases, however, the constant is too wide for other distributions. The Rule of Thumb method reduces the constant used the selection procedure to \\(0.9\\). Silverman chooses this value to avoid losing efficiency on Integrated Mean Squared error at the normal.\rAnother approach is to use a plug-in method. In a plug-in method, the bias of an estimate \\(\\hat g\\) is written as a function of an unknown \\(g\\) and approximated through Taylor series expansions. A preliminary estimate is used to derivate an estimate of the bias (also known as the Integrated mean squared error). The Sheather-Jones method follows the plug-in approach, instead choosing a pilot estimate that best estimates the second derivative of the density function, rather than the density function itself. Sheather-Jones seems to work very well on a broad set of cases, as opposed to the Normal Reference procedure.\rThe methods described so far are based on the approximation to the integrated mean squared error given by the AIMSE. If the AIMSE is not an accurate approximation to the integrated mean squared error, then the smoothing parameter selected by these methods may not work well, even if \\(R_2\\) is accurately estimated using either a reference distribution or a preliminary estimator. The last approach used, Cross-validation, is based on the idea of estimating the IMSE directly, without relying on the AIMSE. This is done by minimizing the objective function\r\\[\\int_{-\\infty}^{\\infty}\\hat p(y; h)^2dy - \\frac{2}{n}\\displaystyle \\sum_{i=1}^{n}\\hat p_{-i}(Y_i; h)\\]\rwhere \\(p_{-i}\\) denotes the kernel density estimator based on all data points except \\(Y_i\\).\rThis idea applies to a wide range of methods, including multivariate density estimation and kernel estimation of a regression function. However, it is known to be more likely to result in poor choices. This is possibly because the method requires numerical minimization of an objective function, which can be a challenging computational problem depending on the shape of the objective function. A more complex objective function can cause overfitting.\nRegardless of bandwidth selectors, each density estimate indicated a rise in deaths per day from 2012-2018.\nSince we know there was a surge in Synthetic opioids and a significant increase in overall deaths, we should look at Fentanyl deaths during this period.\rThe following table shows the proportion of drug overdoses that involved Fentanyl and year.\r\rTable 1: Table 1: Proportion of Deaths related to Fentanyl over Time (Years)\r\r\r\rYear\r\rProportion of Deaths\r\r\r\r\r\r2012\r\r0.04\r\r\r\r2013\r\r0.07\r\r\r\r2014\r\r0.13\r\r\r\r2015\r\r0.26\r\r\r\r2016\r\r0.53\r\r\r\r2017\r\r0.65\r\r\r\r2018\r\r0.75\r\r\r\r\r\n43.64% of deaths over the six-year study were related to Fentanyl deaths. This is an astounding percentage, given that before 2015, three years into the study, Fentanyl was found in less than 30% of deaths. Nearly 74.6% of those who died in 2018 had Fentanyl in their body.\rIf plotted by year, the table would appear to follow logistic growth. We can test this claim by model-checking using nonparametric regression. We can run a nonparametric regression and test whether a linear model fits the data. This can be done by comparing the sum of squared errors. If the linear model fits the data, we expect that \\(SSE_{linear} = SSE_{nonparametric}\\). Another expression of this is\r\\[\\displaystyle\\sum_{i=1}^{n} e_j^2 = \\displaystyle\\sum_{i=1}^{n}(Y_i - \\hat m(X_i))^2\\]\rwhere \\(\\hat m(X_i)\\) is the local-linear kernel estimator of \\(m(\\cdot)\\). Simply put, the residuals between both models should be the same. This implies that the square of the linear model’s residuals is close to zero. Additionally, we can measure the model’s goodness-of-fit by finding the ratio of the linear model’s SSE to the nonparametric model’s SSE. sm.regression features of an argument model = linear, which tests the hypothesis that a linear function explains the relationship between the variables.\rUnfortunately, there’s not much data to back up our claims of nonlinear growth. Let’s change the time parameter from Years to Months for more data points. Let’s also look at the number of deaths over time.\r## Test of linear model: significance = 0\r## Test of linear model: significance = 0\r\r\r\rProportion of Deaths\r\r\rNumber of Deaths(Monthly)\r\r\r\r\r\r\rDegrees-of-Freedom\rError Variance\r\r\r\r7.2\r0.062 (6.2%)\r\r\r\r\r\r\r\rDegrees-of-Freedom\rError Variance\r\r\r\r7.2\r7 (7 deaths)\r\r\r\r\r\r\rThe small p-value suggests that the hypothesis that the linear relationship is appropriate is rejected; that is, the relationship between the proportion of drug-related deaths involving Fentanyl and time is not linear.\n\rTransforming Nonlinear Data\rLet’s look at the growth of Fentanyl Deaths by Number of Deaths. We know that the relationship between Time and Fentanyl Deaths is not linear. The nonparametric estimate seems to follow a sigmoidal curve. However, It doesn’t seem like we’re seeing the deaths hit an asymptote. The number of deaths becomes more disperse during the 60th - 80th months (2017 - 2018).\rFentanyl deaths may follow exponential growth. Let’s test whether this is true or not. A multiplicative model, like an exponential one, corresponds to an additive model on the log scale. This means that we can verify that the data is exponentiated if time follows a linear relationship with the logarithm of deaths.\rThis will also allow us to use linear regression tools to predict the trajectory of Fentanyl deaths.\rLet’s try plotting Time and Fentanyl Deaths, using the logarithm of Fentanyl Deaths.\nlogx \u0026lt;- log(fentrate.ym.ful$x)\rfentrate.ym.ful$logx \u0026lt;- logx\rhead(fentrate.ym.ful)\r## m_y x ind logx\r## 1 2012-01 0 1 -Inf\r## 2 2012-02 1 2 0.00\r## 3 2012-03 1 3 0.00\r## 4 2012-04 1 4 0.00\r## 5 2012-05 1 5 0.00\r## 6 2012-06 2 6 0.69\rWe’ve run into some trouble - we can’t take the log of zero. Let’s just add every entry in the number of deaths column by 1.\n## m_y x ind logx\r## 1 2012-01 0 1 0.00\r## 2 2012-02 1 2 0.69\r## 3 2012-03 1 3 0.69\r## 4 2012-04 1 4 0.69\r## 5 2012-05 1 5 0.69\r## 6 2012-06 2 6 1.10\rGreat - now, let’s see how that turned out.\nnp.fent.full.log \u0026lt;- sm.regression(ind, logx, data=fentrate.ym.ful, col=\u0026quot;green\u0026quot;, pch=19, xlab = \u0026quot;Number of Months\u0026quot;, ylab = \u0026quot;Log Deaths\u0026quot;, model=\u0026quot;linear\u0026quot;)\r## Test of linear model: significance = 0\rWe can rule out that Deaths over Time follows exponential growth. A log transformation linearizes exponential data. We don’t have a linear relationship between the Log of Deaths and the number of months. In fact, the log-deaths model suggests that the growth in the number of deaths is slowing down.\rSo what do we do?\n\rModeling Logistic Growth\rWe know that the growth of deaths is partially exponential. A good part of the log deaths model is linear. We also know that the growth rate of deaths slowed down after 2017.\rRemember how our nonparametric estimate followed a sigmoidal curve? I do too. A sigmoid function follows the same partial exponential growth and decreasing growth rate we see in our data. A common sigmoid function is the logistic function. The logistic growth model can be written as\r\\[P = \\frac{K}{1 + e^{P_o + rt}}\\]\rwhere\n\r\\(P\\): population size (y)\n\r\\(t\\): time (x)\n\r\\(P_o\\): initial population size\n\r\\(r\\): growth rate\n\r\\(K\\): final population size.\n\r\rIn our case, the population size is the number of deaths.\rA logistic growth model can be implemented in R using nls, a non-linear least squares function. Unlike traditional OLS, non-linear least squares need initial starting parameters. The function iteratively evaluates model parameters and selects parameters that minimize model error. Without initial parameters, the function can’t compare the model to other parameters.\rFinding good starting values is very important in nonlinear regression. If starting parameter values are outside of the range of potential parameter values, the algorithm will either fail or return something that doesn’t make sense.\rThe best way to find correct starting values is to “eyeball” the data, or plot the data and find an approximate asymptote, inflection point and slope.\rThis step can be bypassed by using self-starting functions with SSlogis. This function creates an initial estimate of the parameters, which are then plugged into the nonlinear least squares function to find the best-fit logistic equation.\rThe SSlogis function fits a slightly different parameterization of the logistic function\r\\[P = \\frac{Asym}{1 + e^{\\frac{xmid-t}{scal}}}\\]\rwhere \\(Asym = K, xmid = \\frac{-P_o}{r}, scal = \\frac{-1}{r}\\).\rHowever, everything means the same thing. \\(Asym\\) is where the number of deaths plateau, \\(xmid\\) is the x value at the inflection point of the logistic curve and \\(scal\\) is how quickly the number of deaths approaches the asymptote.\r## ## Formula: x ~ SSlogis(ind, Asym, xmid, scal)\r## ## Parameters:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## Asym 66.785 2.883 23.17 \u0026lt; 0.0000000000000002 ***\r## xmid 51.719 1.318 39.25 \u0026lt; 0.0000000000000002 ***\r## scal 8.546 0.984 8.68 0.00000000000033 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 7.2 on 81 degrees of freedom\r## ## Number of iterations to convergence: 2 ## Achieved convergence tolerance: 0.00000212\rI don’t think that’s a bad fit. The curve suggests that deaths should level out at around 65-70 per month.\rLet’s look at the residuals:\n## ## ------\r## Shapiro-Wilk normality test\r## ## data: stdres\r## W = 1, p-value = 0.05\r## ## ## ------\r## ## Runs Test\r## ## data: as.factor(run)\r## Standard Normal = -1, p-value = 0.2\r## alternative hypothesis: two.sided\rThe errors are normally distributed. That’s good, but does this model make sense?\rLet’s try to fit a logistic function on the proportion of deaths over time.\r## ## Formula: prop ~ SSlogis(ind, Asym, xmid, scal)\r## ## Parameters:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## Asym 0.8275 0.0388 21.3 \u0026lt;0.0000000000000002 ***\r## xmid 50.6975 1.6864 30.1 \u0026lt;0.0000000000000002 ***\r## scal 11.8108 1.0914 10.8 \u0026lt;0.0000000000000002 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.071 on 81 degrees of freedom\r## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 0.00000268\r## ## ------\r## Shapiro-Wilk normality test\r## ## data: stdres\r## W = 1, p-value = 0.3\r## ## ## ------\r## ## Runs Test\r## ## data: as.factor(run)\r## Standard Normal = -1, p-value = 0.3\r## alternative hypothesis: two.sided\rThis model isn’t a bad fit either - the errors are also normally distributed. The model suggests that Fentanyl will account for around 80-85% of drug-related deaths. This is a huge percentage - nearly 9 out of 10 drug overdoses involve Fentanyl.\rFor clarity’s sake, let’s summarize what we know so far:\n\rDrug overdose rates have steadily increased since 2012 - more people are dying from drugs.\n\rIn 2012, Fentanyl was involved in less than 10% of drug overdoses. In 2018, Fentanyl was involved in more than 80% of drug overdoses.\n\rThe number of drug-related deaths related to Fentanyl and the proportion of drug-related deaths related to Fentanyl both follow a logistic curve over time.\n\r\rSo what do those curves mean anyway? The asymptote for the number of deaths could suggest that the population that uses Fentanyl is at its maximum. Given that Fentanyl is attributed to most drug deaths, one could expect that Fentanyl is being used or cut with other drugs. Keep in mind that the heroin epidemic and the synthetic opioid epidemic are happening concurrently. As of now, we can’t say much. If anything, we have more questions:\nAre Fentanyl deaths correlated with Heroin deaths or other drug deaths? Is it taken by older people, who may have other comorbidities? How often is Fentanyl taken alone?\rWe didn’t do much Exploratory Data Analysis at all, so let’s revisit these graphs after we explore the data a bit more. If you have the time, check out my second article where I try to answer some of those questions using barplots, heat maps, and other data visualization techniques.\n\r","date":"2020-06-14","permalink":"/post/2020_14_6_opioid_nonparam/","tags":["opioid","logistic","nonparametric"],"title":"Using Nonparametric Methods to Investigate the Opioid Crisis in Connecticut"},{"content":"Thank you for visiting my website! I plan on posting short projects, exercises and things related to statistics and data science on my home page (which you can access by clicking my name in the top-left corner). For more information about me, my interests and why I do what I do, check out my About and Resume page.\rIf you need to look for anything, try my Search page. You can view all of my posts in my Archives page.\rMy website was made using blogdown.\rI tend to abuse the kbd tag. Anything that looks like this will link to another page on my website or a site that has more information on a package I used in a post/project. I thought it would add some personality to the hyperlinks. All other hyperlinks (sources, etc) will look light purple - nothing crazy.\n","date":"2020-05-20","permalink":"/post/first_post/","tags":null,"title":"Hello!"}]